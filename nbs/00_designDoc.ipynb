{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Document\n",
    "\n",
    "> This is the design document for Tango 70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do: \n",
    "\n",
    "Include team suggestions into tango 70b design doc (suggestions at Notion). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tango 70-b Design Document\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Project Overview\n",
    "Project Tango 70-b aims to develop and train the first large language model (LLM) specifically designed for and by Latin Americans, with a focus on Spanish language content. This ambitious project seeks to democratize AI technology in the region by leveraging consumer-grade hardware and innovative training techniques.\n",
    "\n",
    "Key aspects of the project include:\n",
    "\n",
    "1. Training a 70 billion parameter LLM using consumer GPUs (2x NVIDIA RTX 3090).\n",
    "2. Implementing advanced techniques such as QLoRA, FSDP, and Half Quadratic Quantization to enable efficient training on limited hardware.\n",
    "3. Utilizing Latin American-focused datasets, including Wikipedia content, literature, and historical and legal documents.\n",
    "4. Exploring the economic and geopolitical implications of locally developed and hosted AI infrastructure in Latin America.\n",
    "5. Documenting and open-sourcing the entire process to foster AI development in the region.\n",
    "6. Investigating future possibilities for decentralized AI serving and smartphone-based AI applications in Latin America.\n",
    "\n",
    "This project not only aims to create a powerful language model but also to spark discussions about AI sovereignty, economic feasibility, and the potential for Latin America to become a significant player in the global AI landscape. By combining technical innovation with strategic foresight, Tango 70-b seeks to position Argentina and Latin America at the forefront of AI development and application.\n",
    "\n",
    "### Goals and Objectives\n",
    "- Create a Proof of Concept for training highly capable AI on consumer GPUs, defined as an LLM that achieves an MMLU (or other metric) higher than X.\n",
    "- Serve the first AI in Argentina at a cost proportional to local salary ranges/GDP per capita/minimum wage.\n",
    "- Assess the economics of training, hosting, and serving LLMs from and for Latin America.\n",
    "- Gain hands-on experience in training sota LLMs.\n",
    "- Kickstart the discussion on the importance of training and serving AI in Latin America, including social, economic, and geopolitical aspects.\n",
    "- Cement the AI scene in Argentina and Latin America.\n",
    "- Position Argentina as the 4th AI pole globally.\n",
    "\n",
    "### Scope of the Project\n",
    "- Focus on Spanish language and Latin American content.\n",
    "- Utilize consumer-grade hardware for training and serving.\n",
    "- Develop and document the process for future replication.\n",
    "- Explore the specialized LLM space rather than competing with foundation models.\n",
    "\n",
    "### Non-Objectives\n",
    "- Targeting Spanish from Spain or other non-Latin American Spanish-speaking regions.\n",
    "- Competing with big tech companies' centralized serving approach.\n",
    "- Developing foundation models.\n",
    "\n",
    "## 2. Model Architecture\n",
    "\n",
    "### Base Model Selection\n",
    "[To be determined]\n",
    "\n",
    "### Model Size\n",
    "70 billion parameters, potentially using distillation or pruning techniques.\n",
    "\n",
    "### Architecture Modifications\n",
    "Implement QLoRA (Quantized Low-Rank Adaptation) and FSDP (Fully Sharded Data Parallel) to enable training on consumer GPUs.\n",
    "\n",
    "## 3. Training Data\n",
    "\n",
    "### Data Sources\n",
    "- Wikipedia content for Latin American countries (3 levels of links)\n",
    "- Latin American literature and books\n",
    "- Constitutions of Latin American countries\n",
    "- SomosNLP dataset\n",
    "- OpenHermes datasets\n",
    "\n",
    "### Data Preparation and Cleaning\n",
    "we will mimick HuggingFace's FineWeb-Edu approach\n",
    "\n",
    "### Data Format and Structure\n",
    "we will mimick HuggingFace's [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) approach\n",
    "\n",
    "## 4. Training Proces\n",
    "We fine-tune, we do not pre-train.\n",
    "\n",
    "### Hardware Requirements\n",
    "2x NVIDIA RTX 3090 with 24GB VRAM each\n",
    "\n",
    "### Software Stack\n",
    "- PyTorch with FSDP\n",
    "- QLoRA\n",
    "- bitsandbyte\n",
    "- PEFT\n",
    "- Transformers\n",
    "- Accelerate\n",
    "\n",
    "### Training Techniques\n",
    "\n",
    "#### 1. Quantization (QLoRA)\n",
    "- Utilize 4-bit quantization to reduce model memory footprint by ~75%\n",
    "- Implement QLoRA, combining quantization with Low-Rank Adaptation\n",
    "- Use a 4-bit quantized frozen base model with trainable low-rank adapters\n",
    "- Employ 4-bit NormalFloat (NF4) data type for normally distributed weights\n",
    "- Implement double quantization to further reduce memory usage\n",
    "\n",
    "#### 2. Fully Sharded Data Parallel (FSDP)\n",
    "- Shard model parameters, optimizer states, and gradients across multiple GPUs\n",
    "- Perform all-gather operations during forward pass\n",
    "- Use reduce-scatter operations for gradient synchronization during backward pass\n",
    "\n",
    "#### 3. Gradient Checkpointing\n",
    "- Trade computation for memory by not storing all activations\n",
    "- Save checkpoints and recompute activations as needed during backward pass\n",
    "\n",
    "#### 4. CPU Offloading\n",
    "- Store some model parameters and optimizer states in CPU RAM when not in use\n",
    "- Significantly reduce GPU memory requirements\n",
    "\n",
    "#### 5. Flash Attention\n",
    "- Implement optimized attention computation using memory-efficient CUDA kernels\n",
    "\n",
    "#### 6. Half Quadratic Quantization (HQQ)\n",
    "- Utilize HQQ as an alternative or complement to standard 4-bit quantization\n",
    "- Minimize weight errors using a sparsity-promoting loss function\n",
    "- Employ a half-quadratic solver for efficient optimization\n",
    "\n",
    "#### Challenges and Considerations\n",
    "- Training will be slower compared to data center GPUs\n",
    "- Potential loss in model quality due to quantization\n",
    "- Balancing sequence length and batch size within memory constraints\n",
    "- Careful management of quantization state and FSDP synchronization\n",
    "\n",
    "\n",
    "\n",
    "## 5. Evaluation Metrics\n",
    "\n",
    "### Performance Metrics\n",
    "We will use a variety of benchmarks to evaluate our model's performance. We will not be able to\n",
    "compete (yet) with the leading models like claude, or gemini, but if we can get to within 10% of their perf, that's good enough as a starting point. \n",
    "\n",
    "\n",
    "### Evaluation Datasets and Benchmarks\n",
    "We will use the following datasets and benchmarks for evaluation:\n",
    "\n",
    "1. MMBench v1.1\n",
    "2. MMStar\n",
    "3. MMMU_VAL\n",
    "4. MathVista_MINI\n",
    "5. HallusionBench\n",
    "6. AI2D_TEST\n",
    "7. OCRBench\n",
    "8. MMVet\n",
    "\n",
    "Here are the benchmark scores of leading models for reference:\n",
    "\n",
    "| Benchmark       | GPT-4o-20240513 | Claude3.5-Sonnet | Gemini-1.5-Pro | GPT-4v-20240409 |\n",
    "|-----------------|-----------------|-------------------|-----------------|------------------|\n",
    "| Overall Rank    | 1               | 2                 | 3               | 4                |\n",
    "| Avg. Score      | 69.9            | 67.9              | 64.4            | 63.5             |\n",
    "| MMBench v1.1    | 82.2            | 78.5              | 73.9            | 79.8             |\n",
    "| MMStar          | 63.9            | 62.2              | 59.1            | 56.0             |\n",
    "| MMMU_VAL        | 69.2            | 65.9              | 60.6            | 61.7             |\n",
    "| MathVista_MINI  | 61.3            | 61.6              | 57.7            | 54.7             |\n",
    "| HallusionBench  | 55.0            | 49.9              | 45.6            | 43.9             |\n",
    "| AI2D_TEST       | 84.6            | 80.2              | 79.1            | 78.6             |\n",
    "| OCRBench        | 736             | 788               | 754             | 656              |\n",
    "| MMVet           | 69.1            | 66                | 64              | 67.5             |\n",
    "\n",
    "\n",
    "## 6. Ethical Considerations\n",
    "None to begin with, but we aim to open up the discussion by reaching out to researchers and thinkers. \n",
    "\n",
    "### Bias Mitigation Strategies\n",
    "[To be developed]\n",
    "\n",
    "### Safety Measures\n",
    "None, we don't go into that hype and we don't want regulatory capture. \n",
    "+ guardrails can be added to the systems that build upon tango (e.g., nvidia's nemo guardrails)\n",
    "\n",
    "### Privacy Considerations\n",
    "[To be developed]\n",
    "\n",
    "## 7. Deployment and Inference\n",
    "\n",
    "### Deployment Environment\n",
    "Explore decentralized and distributed hosting options suitable for the (mostly undeveloped) Latin American infrastructure.\n",
    "\n",
    "### Inference Optimization\n",
    "Implement quantization techniques like Half Quadratic Quantization (HQQ) for efficient inference.\n",
    "\n",
    "### Scaling Considerations\n",
    "see `Future Considerations` on architectures that scale sub-linearly with context lenght. \n",
    "\n",
    "## 8. Maintenance and Iteration\n",
    "[To be determined]\n",
    "\n",
    "### Monitoring Plan\n",
    "Weights & Biases\n",
    "\n",
    "### Version Control\n",
    "Github\n",
    "\n",
    "## 9. Timeline and Milestones\n",
    "- 2024\n",
    "    * test runs until september the 1st\n",
    "    * data scrapping throught the week of september the 2nd\n",
    "    * train tango-70b v0.0.1 over the september 7th-8th weekend\n",
    "    * soft release (huggingface, linkedin) on september 9th\n",
    "    * full release on october the 1st\n",
    "- 2025 \n",
    "    * [To be determined]\n",
    "\n",
    "## 10. Resources and Budget\n",
    "\n",
    "### Team Composition\n",
    "lexia x sandbox\n",
    "\n",
    "### Computing Resources\n",
    "Dual RTX 3090 setup, with potential for cloud resources (e.g., Runpod Community Cloud at ~$0.60/hour)\n",
    "\n",
    "### Estimated Costs\n",
    "None if fully local, [To be calculated] if partially on cloud\n",
    "\n",
    "## 11. Looking Forward\n",
    "\n",
    "### Future Considerations\n",
    "- Explore more cost-effective architectures (sub-linear scaling of context) such as State Space Models (Mamba and Jamba) and RNNs (RWKV)\n",
    "- Investigate the potential for running AI on smartphones in Latin America over the next 2-4 years. \n",
    "\n",
    "## 12. Communication and Outreach\n",
    "\n",
    "### Technical Paper/Post\n",
    "Document the entire process, challenges, and solutions for the Latin American AI community.\n",
    "\n",
    "### News article\n",
    "\"Argentinians build the first AI of Latin America\" - highlighting cultural inclusions (e.g., writings from San Martín, Cortázar), societal impact, geopolitical derivations.\n",
    "\n",
    "\n",
    "### \"The Geopolitics of LLMs and can Argentina be the next big AI player?\"\n",
    "discussing the potential and challenges for Argentina in the global AI landscape.\n",
    "\n",
    "- (some) of the questions to be answered: \n",
    "    * Are there political or geographical reasons for argentina and latam to develop and host it's own AI?\n",
    "    * Can foreign AI properly represent our values and interests as latinoamericans?\n",
    "    * Can foreignly trained and/or served AI be properly in control of latam owners?\n",
    "\n",
    "### \"The economics of hosting and serving LLMs in Latin America\"\n",
    "sxploring the feasibility and implications of local (and potentially distributed and decentralized) AI infrastructure.\n",
    "\n",
    "- (some) of the questions to be answered: \n",
    "    * Is it convenient to host llms on latin america? why not just pay the us, france or china to serve them for us?\n",
    "    * If it's not economically convenient to serve them here, why is it that the case? are we missing a hidden cost or benefit? e.g., progressively buying/getting the compute to build AI infrastructure in latin america. \n",
    "    * If most AI services cost 20 USD per month, why should latam countries pay that? can we get a business going by charging the equivalent on % of minimum wage? --> e.g. if 20USD per month is say 2% of minimum wage in the US, can we get a business going by charging 2% of Argentina/Latam's minimum wage? buying compute is certainly the same cost -given we can get some gpus on our hands- but certainly the cost of running the compute should be cheaper. (and most likely we should go for something like ROCm instead of CUDA)\n",
    "    * Are there economic reasons/drivers to develp AI infrastructure in Latam? is it sortof the equivalent to energetic selfsustainability? What other drivers are there? geopolitical ones? War/Military ones? what happens if a latam country goes to war or get's into political conflict with it's provider of AI infrastructure? isn't it too risky to not have our own infrastructure?\n",
    "    * Is de-centralization the solution to cheap AI serving? since we don't have the money to pay for everyones compute, then we should just use the decentralized and distributed compute on everyone's phone? Can we expect to have enough power on our smartphones to run useful AI on our phones? if not in 2 years (Moore's law?), in how many years? Can Argentina and Latam prepare for when this happens?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
