{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: notes on tango's training process\n",
    "output-file: training.html\n",
    "title: Training\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a 70 billion parameter (70B) language model at home requires combining several advanced techniques to overcome memory and computational constraints. \n",
    "\n",
    "## Quantization:\n",
    "The first key technique is quantization, specifically 4-bit quantization. This reduces the memory footprint of the model by approximately 75% compared to 16-bit floating point representation1. QLoRA (Quantized Low-Rank Adaptation) is a method that enables fine-tuning of quantized large language models2. It uses a 4-bit quantized frozen base model and adds trainable low-rank adapters.\n",
    "\n",
    "## Fully Sharded Data Parallel (FSDP):\n",
    "FSDP is a distributed training technique that shards model parameters, optimizer states, and gradients across multiple GPUs3. This allows training of models larger than what can fit on a single GPU. FSDP works by:\n",
    "Sharding model parameters across GPUs\n",
    "Performing all-gather operations to collect full parameters during forward pass\n",
    "Using reduce-scatter operations to synchronize gradients during backward pass\n",
    "\n",
    "## Gradient Checkpointing:\n",
    "This technique trades computation for memory by not storing all activations. Instead, it saves checkpoints and recomputes activations as needed during the backward pass4.\n",
    "\n",
    "## CPU Offloading:\n",
    "Some model parameters and optimizer states can be offloaded to CPU RAM when not in use, further reducing GPU memory requirements4.\n",
    "\n",
    "## Flash Attention:\n",
    "This is an optimized attention implementation that reduces memory usage and improves computational efficiency4.\n",
    "Combining these techniques, it becomes possible to train a 70B model on consumer-grade hardware. For example, using QLoRA with 4-bit quantization reduces the model size from 140GB (70B 2 bytes for 16-bit) to about 35GB4. This can then be sharded across multiple GPUs using FSDP.\n",
    "\n",
    "The process would look something like this:\n",
    "\n",
    "1. Load the pre-trained 70B model and quantize it to 4-bit precision.\n",
    "2. Add trainable LoRA adapters to the quantized model.\n",
    "3. Wrap the model with FSDP, using an appropriate auto-wrap policy to optimize sharding.\n",
    "4. Use gradient checkpointing and CPU offloading to further manage memory usage.\n",
    "5. Implement Flash Attention for efficient attention computation.\n",
    "6. Train the model using a distributed data loader and optimizer.\n",
    "\n",
    "It's important to note that while this setup allows training on consumer hardware, it comes with trade-offs. Training will be slower compared to using data center GPUs, and there may be some loss in model quality due to quantization. However, this approach democratizes access to large language model training, enabling researchers and enthusiasts to experiment with state-of-the-art models on more accessible hardware4.\n",
    "\n",
    "This combination of techniques represents a significant advancement in making large-scale AI research more accessible, potentially leading to more diverse contributions to the field.\n",
    "\n",
    "\n",
    "\n",
    "## Half Quadratic Quantization\n",
    "\n",
    "Half Quadratic Quantization (HQQ) is an advanced quantization technique for large machine learning models that aims to achieve high-quality quantization without the need for calibration data. Here's a breakdown of the key aspects of HQQ:\n",
    "Objective:\n",
    "HQQ aims to minimize errors in the weights of the model rather than layer activations. It uses a sparsity-promoting loss function to model outliers through a hyper-Laplacian distribution, which better captures the heavy-tailed nature of outlier errors compared to squared error approaches1.\n",
    "Optimization Formulation:\n",
    "HQQ uses a robust optimization formulation to find the quantization parameters (zero-point z and scaling s). The objective is to minimize a sparsity-promoting loss function φ() between the original weights W and their dequantized version1:\n",
    "argmin(z,s) φ(W - Q^(-1)z,s(Q_z,s(W)))\n",
    "Where Q_z,s() is the quantization operator and Q^(-1)z,s() is the de-quantization operator.\n",
    "Half-Quadratic Solver:\n",
    "To solve this non-convex problem, HQQ adopts a Half-Quadratic solver by introducing an extra variable W_e. This allows splitting the main problem into easier-to-solve sub-problems1.\n",
    "Sub-problems:\n",
    "The optimization is done through alternating optimization of two sub-problems:\n",
    "a) Updating W_e using a generalized soft-thresholding operator\n",
    "b) Updating the zero-point z by minimizing the squared error between the quantized and target weights1\n",
    "Efficiency:\n",
    "Unlike methods that use gradient descent with autograd, HQQ relies on closed-form solutions. This allows all calculations to be run in inference mode with half-precision, resulting in significant speed-ups (over 100x faster vs. autograd for quantizing Llama-2-7B)1.\n",
    "6. Performance:\n",
    "HQQ has shown competitive performance with calibration-based methods like GPTQ and AWQ, while being much faster. For example, it can process the Llama-2-70B model in just a few minutes1.\n",
    "7. Flexibility:\n",
    "HQQ can be used for various bit-widths, including extreme low-bit quantization (e.g., 2-bit), and has shown good results across different model sizes and applications1.\n",
    "In summary, Half Quadratic Quantization offers a fast, calibration-free approach to quantizing large language models while maintaining competitive performance with more computationally expensive calibration-based methods. This makes it particularly useful for quickly deploying or fine-tuning large models on resource-constrained hardware.\n",
    "\n",
    "\n",
    "## QLoRA\n",
    "QLoRA (Quantized Low-Rank Adaptation) is an efficient finetuning approach for large language models (LLMs) that significantly reduces memory usage while maintaining performance. Here's a thorough explanation of QLoRA based on the arXiv paper1:\n",
    "Core Concept:\n",
    "QLoRA combines quantization and Low-Rank Adaptation (LoRA) to enable finetuning of large models on limited hardware. It allows finetuning a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.\n",
    "Key Components:\n",
    "a) 4-bit Quantization: The pretrained model is quantized to 4 bits, reducing memory usage by about 75% compared to 16-bit models.\n",
    "b) LoRA: Trainable low-rank adapters are added to the frozen, quantized base model.\n",
    "c) Backpropagation: Gradients are backpropagated through the quantized model into the LoRA adapters.\n",
    "Technical Innovations:\n",
    "a) 4-bit NormalFloat (NF4): A new data type optimized for normally distributed weights, which is information-theoretically optimal for such distributions.\n",
    "b) Double Quantization: Quantizing the quantization constants themselves to further reduce memory footprint.\n",
    "c) Paged Optimizers: A technique to manage memory spikes during training.\n",
    "\n",
    "Scalability:\n",
    "QLoRA enables finetuning of models at scales previously infeasible with regular finetuning methods (e.g., 33B and 65B parameter models).\n",
    "\n",
    "QLoRA democratizes access to large language model training, enabling researchers and enthusiasts to experiment with state-of-the-art models on more accessible hardware.\n",
    "In summary, QLoRA represents a significant advancement in making large-scale AI research more accessible by combining efficient quantization techniques with low-rank adaptation, allowing for the finetuning of massive language models on consumer-grade hardware while maintaining high performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
