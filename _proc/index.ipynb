{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: tango70b\n",
    "output-file: index.html\n",
    "title: tango\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.10 or later\n",
    "- CUDA-compatible GPU (CUDA 11.8 recommended)\n",
    "- Git\n",
    "\n",
    "## Installation\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/your-username/your-repo-name.git\n",
    "   cd your-repo-name\n",
    "   ```\n",
    "\n",
    "2. Create and activate a virtual environment (optional but recommended):\n",
    "   ```bash\n",
    "   python3 -m venv venv\n",
    "   source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
    "   ```\n",
    "\n",
    "3. Install PyTorch with CUDA support:\n",
    "   ```bash\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "   ```\n",
    "\n",
    "4. Install the required packages:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "5. Install HQQ:\n",
    "   ```bash\n",
    "   git clone https://github.com/mobiusml/hqq.git\n",
    "   cd hqq\n",
    "   pip install .\n",
    "   cd hqq/kernels\n",
    "   python setup_cuda.py install\n",
    "   cd ../..\n",
    "   ```\n",
    "\n",
    "## Usage\n",
    "\n",
    "To run the training script, use the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python train.py [arguments]\n",
    "\n",
    "```\n",
    "\n",
    "### Important Arguments\n",
    "\n",
    "- `--world_size`: Number of GPUs to use. -1 = all available GPUs.\n",
    "- `--train_type`: Training type (e.g., \"qlora\", \"full\", \"lora\", \"custom_qlora\")\n",
    "- `--batch_size`: Batch size per GPU\n",
    "- `--num_epochs`: Number of training epochs\n",
    "- `--dataset`: Dataset to use (e.g., \"alpaca\", \"alpaca_sample\", \"dummy\")\n",
    "- `--model_name`: Model to train (e.g., \"meta-llama/Llama-2-7b-hf\")\n",
    "- `--precision`: Training precision (e.g., \"bf16\", \"fp32\")\n",
    "\n",
    "For a full list of arguments and their descriptions, run:\n",
    "\n",
    "```bash\n",
    "python train.py --help\n",
    "```\n",
    "\n",
    "\n",
    "### Quick run\n",
    "\n",
    "```bash\n",
    "python train.py \\\n",
    "--model_name meta-llama/Llama-2-70b-hf \\\n",
    "--batch_size 2 \\\n",
    "--context_length 512 \\\n",
    "--precision bf16 \\\n",
    "--train_type qlora \\\n",
    "--use_gradient_checkpointing true \\\n",
    "--use_cpu_offload true \\\n",
    "--dataset alpaca \\\n",
    "--reentrant_checkpointing true\n",
    "```\n",
    "\n",
    "\n",
    "this uses over 130gb of cpu ram, but note you can use swap memory.\n",
    "note that if you don't use cpu offloading (`use_cpu_offload false`), ram usage will be much lower. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Running Jupyter Notebook\n",
    "\n",
    "To run Jupyter Notebook for interactive development:\n",
    "\n",
    "1. Start Jupyter Notebook:\n",
    "   ```bash\n",
    "   jupyter notebook\n",
    "   ```\n",
    "\n",
    "2. Open your browser and go to `http://localhost:8888`\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Make sure your CUDA version is compatible with your GPU. You can check supported architectures and adjust the `TORCH_CUDA_ARCH_LIST` environment variable if needed.\n",
    "- The project uses specific versions of transformers library, excluding versions 4.38.* and 4.39.*. If you encounter issues, you may need to adjust the version constraints.\n",
    "- For optimal performance, ensure you have the latest NVIDIA drivers installed for your GPU.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If you encounter any issues with CUDA or GPU support, make sure:\n",
    "1. Your NVIDIA drivers are up to date\n",
    "2. The installed PyTorch version matches your CUDA version\n",
    "3. Your GPU is CUDA-compatible and supported by the installed PyTorch version\n",
    "\n",
    "## Contributing\n",
    "\n",
    "ask the pibes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
